{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44a1a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0000\n",
      "Random Forest Unique Predictions: [0 1]\n",
      "Random Forest Prediction Distribution: 0    284862\n",
      "1    284840\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Logistic Regression Accuracy: 0.5120\n",
      "Logistic Regression Unique Predictions: [0 1]\n",
      "Logistic Regression Prediction Distribution: 0    318963\n",
      "1    250739\n",
      "Name: count, dtype: int64\n",
      "\n",
      "HDBSCAN Accuracy: 0.5315\n",
      "HDBSCAN Unique Predictions: [0 1]\n",
      "HDBSCAN Prediction Distribution: 0    440996\n",
      "1    128706\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Isolation Forest Accuracy: 0.5000\n",
      "Isolation Forest Unique Predictions: [0]\n",
      "Isolation Forest Prediction Distribution: 0    569702\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define paths\n",
    "MODEL_PATH = \"/home/students/Documents/AML CFT_dashboard_project/models\"\n",
    "DATA_PATH = \"/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed\"\n",
    "TOP_FEATURES = joblib.load(f\"{DATA_PATH}/top_features.pkl\")\n",
    "\n",
    "# Load saved models\n",
    "models = {\n",
    "    \"Random Forest\": joblib.load(f\"{MODEL_PATH}/random_forest.pkl\"),\n",
    "    \"Logistic Regression\": joblib.load(f\"{MODEL_PATH}/logistic_regression.pkl\"),\n",
    "    \"HDBSCAN\": joblib.load(f\"{MODEL_PATH}/hdbscan_model.pkl\"),\n",
    "    \"Isolation Forest\": joblib.load(f\"{MODEL_PATH}/isolation_forest.pkl\")\n",
    "}\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv(f\"{DATA_PATH}/X_test.csv\")\n",
    "y_test = pd.read_csv(f\"{DATA_PATH}/y_test.csv\", header=None, skiprows=1, dtype={0: int})[0]\n",
    "\n",
    "# Ensure X_test matches TOP_FEATURES and align lengths\n",
    "X_test = X_test[TOP_FEATURES]\n",
    "if len(X_test) != len(y_test):\n",
    "    min_length = min(len(X_test), len(y_test))\n",
    "    X_test = X_test.iloc[:min_length]\n",
    "    y_test = y_test.iloc[:min_length]\n",
    "    print(f\"Trimmed data to match lengths: {min_length}\")\n",
    "\n",
    "# Make predictions with each model\n",
    "predictions = {}\n",
    "for model_name, model in models.items():\n",
    "    if model_name == \"HDBSCAN\":\n",
    "        labels = model.fit_predict(X_test)\n",
    "        predictions[model_name] = (labels == -1).astype(int)\n",
    "    else:\n",
    "        if model_name == \"Isolation Forest\":\n",
    "            # Use first 2 numerical features (Amount, Recipient_diversity) as per dashboard\n",
    "            numerical_cols = ['Amount', 'Recipient_diversity', 'Sender_diversity', 'Daily_frequency',\n",
    "                              'Avg_velocity', 'Total_inflow', 'Total_outflow', 'Inflow_Outflow_Ratio',\n",
    "                              'Txn_sequence', 'Rolling_avg_amt', 'Weekday', 'Day', 'Month']\n",
    "            X_if = X_test[numerical_cols[:2]].values\n",
    "            predictions[model_name] = np.where(model.predict(X_if) == -1, 1, 0)\n",
    "        else:\n",
    "            predictions[model_name] = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy (if y_test is available and lengths match)\n",
    "    if y_test is not None and len(y_test) == len(predictions[model_name]):\n",
    "        acc = accuracy_score(y_test, predictions[model_name])\n",
    "        print(f\"{model_name} Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Print unique predictions to check for uniformity\n",
    "    unique_preds = np.unique(predictions[model_name])\n",
    "    print(f\"{model_name} Unique Predictions: {unique_preds}\")\n",
    "    print(f\"{model_name} Prediction Distribution: {pd.Series(predictions[model_name]).value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60487f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/students/Documents/AML CFT_dashboard_project/models/isolation_forest.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "\n",
    "X_train = pd.read_csv(\"/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/X_train.csv\")\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "iso_forest.fit(X_train[TOP_FEATURES])  # Use all 20 features\n",
    "joblib.dump(iso_forest, \"/home/students/Documents/AML CFT_dashboard_project/models/isolation_forest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c38a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0000\n",
      "Random Forest Unique Predictions: [0 1]\n",
      "Random Forest Prediction Distribution: 0    284862\n",
      "1    284840\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Logistic Regression Accuracy: 0.5120\n",
      "Logistic Regression Unique Predictions: [0 1]\n",
      "Logistic Regression Prediction Distribution: 0    318963\n",
      "1    250739\n",
      "Name: count, dtype: int64\n",
      "\n",
      "HDBSCAN Accuracy: 0.5315\n",
      "HDBSCAN Unique Predictions: [0 1]\n",
      "HDBSCAN Prediction Distribution: 0    440996\n",
      "1    128706\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Isolation Forest Accuracy: 0.5381\n",
      "Isolation Forest Unique Predictions: [0 1]\n",
      "Isolation Forest Prediction Distribution: 0    512590\n",
      "1     57112\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define paths\n",
    "MODEL_PATH = \"/home/students/Documents/AML CFT_dashboard_project/models\"\n",
    "DATA_PATH = \"/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed\"\n",
    "TOP_FEATURES = joblib.load(f\"{DATA_PATH}/top_features.pkl\")\n",
    "\n",
    "# Load saved models\n",
    "models = {\n",
    "    \"Random Forest\": joblib.load(f\"{MODEL_PATH}/random_forest.pkl\"),\n",
    "    \"Logistic Regression\": joblib.load(f\"{MODEL_PATH}/logistic_regression.pkl\"),\n",
    "    \"HDBSCAN\": joblib.load(f\"{MODEL_PATH}/hdbscan_model.pkl\"),\n",
    "    \"Isolation Forest\": joblib.load(f\"{MODEL_PATH}/isolation_forest.pkl\")\n",
    "}\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv(f\"{DATA_PATH}/X_test.csv\")\n",
    "y_test = pd.read_csv(f\"{DATA_PATH}/y_test.csv\", header=None, skiprows=1, dtype={0: int})[0]\n",
    "\n",
    "# Ensure X_test matches TOP_FEATURES and align lengths\n",
    "X_test = X_test[TOP_FEATURES]\n",
    "if len(X_test) != len(y_test):\n",
    "    min_length = min(len(X_test), len(y_test))\n",
    "    X_test = X_test.iloc[:min_length]\n",
    "    y_test = y_test.iloc[:min_length]\n",
    "    print(f\"Trimmed data to match lengths: {min_length}\")\n",
    "\n",
    "# Make predictions with each model\n",
    "predictions = {}\n",
    "for model_name, model in models.items():\n",
    "    if model_name == \"HDBSCAN\":\n",
    "        labels = model.fit_predict(X_test)\n",
    "        predictions[model_name] = (labels == -1).astype(int)\n",
    "    else:\n",
    "        predictions[model_name] = model.predict(X_test)\n",
    "        if model_name == \"Isolation Forest\":\n",
    "            # Use all features since the model was retrained with TOP_FEATURES\n",
    "            predictions[model_name] = np.where(model.predict(X_test) == -1, 1, 0)\n",
    "\n",
    "    # Calculate accuracy (if y_test is available and lengths match)\n",
    "    if y_test is not None and len(y_test) == len(predictions[model_name]):\n",
    "        acc = accuracy_score(y_test, predictions[model_name])\n",
    "        print(f\"{model_name} Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Print unique predictions to check for uniformity\n",
    "    unique_preds = np.unique(predictions[model_name])\n",
    "    print(f\"{model_name} Unique Predictions: {unique_preds}\")\n",
    "    print(f\"{model_name} Prediction Distribution: {pd.Series(predictions[model_name]).value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436790b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the raw dataset\n",
    "data = pd.read_csv(\"/home/students/Documents/AML CFT_dashboard_project/data/raw/SAML-D.csv\")\n",
    "\n",
    "# Preprocess the data to match cleaned_data.csv structure\n",
    "data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "data['Time'] = pd.to_datetime(data['Time'], format='%H:%M:%S', errors='coerce')\n",
    "data = data.sort_values(by=['Sender_account', 'Date', 'Time'])\n",
    "data['Total_inflow'] = data.groupby('Receiver_account')['Amount'].cumsum()\n",
    "data['Total_outflow'] = data.groupby('Sender_account')['Amount'].cumsum()\n",
    "data['Inflow_Outflow_Ratio'] = data['Total_inflow'] / (data['Total_outflow'] + 1e-6)\n",
    "\n",
    "# Calculate diversity using a rolling window with custom apply\n",
    "def rolling_nunique(x, window=100):\n",
    "    return x.rolling(window=window, min_periods=1).apply(lambda y: y.nunique(), raw=False)\n",
    "\n",
    "data['Recipient_diversity'] = data.groupby('Sender_account')['Receiver_account'].apply(\n",
    "    lambda x: rolling_nunique(x)\n",
    ").reset_index(level=0, drop=True)\n",
    "\n",
    "data['Sender_diversity'] = data.groupby('Receiver_account')['Sender_account'].apply(\n",
    "    lambda x: rolling_nunique(x)\n",
    ").reset_index(level=0, drop=True)\n",
    "\n",
    "data['Daily_frequency'] = data.groupby(['Sender_account', 'Date']).transform('size')\n",
    "data['Avg_velocity'] = data.groupby('Sender_account')['Daily_frequency'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "data['Txn_sequence'] = data.groupby('Sender_account').cumcount() + 1\n",
    "data['Rolling_avg_amt'] = data.groupby('Sender_account')['Amount'].rolling(\n",
    "    window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "data['Hour'] = data['Time'].dt.hour\n",
    "data['Minute'] = data['Time'].dt.minute\n",
    "data['Weekday'] = data['Date'].dt.weekday\n",
    "data['Day'] = data['Date'].dt.day\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data = data.drop(columns=['Time', 'Laundering_type'])\n",
    "data['Sender_account'] = data['Sender_account'].astype('int32')\n",
    "data['Receiver_account'] = data['Receiver_account'].astype('int32')\n",
    "data['Amount'] = data['Amount'].astype('float32')\n",
    "if 'Is_laundering' in data.columns:\n",
    "    data['Is_laundering'] = data['Is_laundering'].astype('int8')\n",
    "for col in ['Recipient_diversity', 'Sender_diversity', 'Daily_frequency', 'Avg_velocity',\n",
    "            'Total_inflow', 'Total_outflow', 'Inflow_Outflow_Ratio', 'Txn_sequence', 'Rolling_avg_amt']:\n",
    "    data[col] = data[col].astype('float32')\n",
    "data = pd.get_dummies(data, columns=['Payment_currency', 'Received_currency', 'Sender_bank_location', 'Receiver_bank_location', 'Payment_type'], drop_first=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Is_laundering', axis=1)\n",
    "y = data['Is_laundering']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)\n",
    "\n",
    "# Save the processed files\n",
    "X_train.to_csv(\"/home/students/Documents/AML CFT_dashboard_project/data/processed/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"/home/students/Documents/AML CFT_dashboard_project/data/processed/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"/home/students/Documents/AML CFT_dashboard_project/data/processed/y_train.csv\", index=False, header=True)\n",
    "y_test.to_csv(\"/home/students/Documents/AML CFT_dashboard_project/data/processed/y_test.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims_cv_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
