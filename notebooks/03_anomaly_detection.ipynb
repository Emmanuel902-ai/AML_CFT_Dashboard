{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a42153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/students/.local/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /home/students/.local/lib/python3.8/site-packages (1.3.2)\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Requirement already satisfied: matplotlib in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (3.7.5)\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.1.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting dash\n",
      "  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: joblib in /home/students/.local/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/students/.local/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/students/.local/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/students/.local/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/students/.local/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/students/.local/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: imbalanced-learn in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from imblearn) (0.12.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-1.42.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in /home/students/.local/lib/python3.8/site-packages (from dash) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug<3.1 in /home/students/.local/lib/python3.8/site-packages (from dash) (3.0.6)\n",
      "Requirement already satisfied: importlib-metadata in /home/students/.local/lib/python3.8/site-packages (from dash) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from dash) (4.12.2)\n",
      "Requirement already satisfied: requests in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from dash) (2.32.3)\n",
      "Collecting retrying (from dash)\n",
      "  Downloading retrying-1.4.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: nest-asyncio in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from dash) (1.6.0)\n",
      "Requirement already satisfied: setuptools in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from dash) (75.1.0)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from Flask<3.1,>=1.0.4->dash) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/students/.local/lib/python3.8/site-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/students/.local/lib/python3.8/site-packages (from Flask<3.1,>=1.0.4->dash) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/students/.local/lib/python3.8/site-packages (from Flask<3.1,>=1.0.4->dash) (1.8.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/students/.local/lib/python3.8/site-packages (from importlib-metadata->dash) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/students/.local/lib/python3.8/site-packages (from Werkzeug<3.1->dash) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from requests->dash) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from requests->dash) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from requests->dash) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from requests->dash) (2025.4.26)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading plotly-6.1.2-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-1.42.1-py3-none-any.whl (359 kB)\n",
      "Downloading retrying-1.4.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: retrying, narwhals, plotly, dash, imblearn\n",
      "Successfully installed dash-3.0.4 imblearn-0.0 narwhals-1.42.1 plotly-6.1.2 retrying-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn imblearn matplotlib plotly dash joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf35833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hdbscan\n",
      "  Downloading hdbscan-0.8.40-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /home/students/miniconda3/envs/aims_cv_2025/lib/python3.8/site-packages (from hdbscan) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/students/.local/lib/python3.8/site-packages (from hdbscan) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/students/.local/lib/python3.8/site-packages (from hdbscan) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/students/.local/lib/python3.8/site-packages (from hdbscan) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/students/.local/lib/python3.8/site-packages (from scikit-learn>=0.20->hdbscan) (3.5.0)\n",
      "Downloading hdbscan-0.8.40-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hdbscan\n",
      "Successfully installed hdbscan-0.8.40\n"
     ]
    }
   ],
   "source": [
    "!pip install hdbscan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed31f96",
   "metadata": {},
   "source": [
    "1. Preprocessing Pipeline\n",
    "\n",
    "This updates the preprocessing to exclude Is_laundering from scaling while keeping it as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054439cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved.\n",
      "Saved data shape: (9504852, 83)\n",
      "Saved data columns: ['Date', 'Sender_account', 'Receiver_account', 'Amount', 'Is_laundering', 'Recipient_diversity', 'Sender_diversity', 'Daily_frequency', 'Avg_velocity', 'Total_inflow', 'Total_outflow', 'Inflow_Outflow_Ratio', 'Txn_sequence', 'Rolling_avg_amt', 'Hour', 'Minute', 'Weekday', 'Day', 'Month', 'Payment_currency_Dirham', 'Payment_currency_Euro', 'Payment_currency_Indian rupee', 'Payment_currency_Mexican Peso', 'Payment_currency_Moroccan dirham', 'Payment_currency_Naira', 'Payment_currency_Pakistani rupee', 'Payment_currency_Swiss franc', 'Payment_currency_Turkish lira', 'Payment_currency_UK pounds', 'Payment_currency_US dollar', 'Payment_currency_Yen', 'Received_currency_Dirham', 'Received_currency_Euro', 'Received_currency_Indian rupee', 'Received_currency_Mexican Peso', 'Received_currency_Moroccan dirham', 'Received_currency_Naira', 'Received_currency_Pakistani rupee', 'Received_currency_Swiss franc', 'Received_currency_Turkish lira', 'Received_currency_UK pounds', 'Received_currency_US dollar', 'Received_currency_Yen', 'Sender_bank_location_Austria', 'Sender_bank_location_France', 'Sender_bank_location_Germany', 'Sender_bank_location_India', 'Sender_bank_location_Italy', 'Sender_bank_location_Japan', 'Sender_bank_location_Mexico', 'Sender_bank_location_Morocco', 'Sender_bank_location_Netherlands', 'Sender_bank_location_Nigeria', 'Sender_bank_location_Pakistan', 'Sender_bank_location_Spain', 'Sender_bank_location_Switzerland', 'Sender_bank_location_Turkey', 'Sender_bank_location_UAE', 'Sender_bank_location_UK', 'Sender_bank_location_USA', 'Receiver_bank_location_Austria', 'Receiver_bank_location_France', 'Receiver_bank_location_Germany', 'Receiver_bank_location_India', 'Receiver_bank_location_Italy', 'Receiver_bank_location_Japan', 'Receiver_bank_location_Mexico', 'Receiver_bank_location_Morocco', 'Receiver_bank_location_Netherlands', 'Receiver_bank_location_Nigeria', 'Receiver_bank_location_Pakistan', 'Receiver_bank_location_Spain', 'Receiver_bank_location_Switzerland', 'Receiver_bank_location_Turkey', 'Receiver_bank_location_UAE', 'Receiver_bank_location_UK', 'Receiver_bank_location_USA', 'Payment_type_Cash Deposit', 'Payment_type_Cash Withdrawal', 'Payment_type_Cheque', 'Payment_type_Credit card', 'Payment_type_Cross-border', 'Payment_type_Debit card']\n",
      "Missing values: Date                            0\n",
      "Sender_account                  0\n",
      "Receiver_account                0\n",
      "Amount                          0\n",
      "Is_laundering                   0\n",
      "                               ..\n",
      "Payment_type_Cash Withdrawal    0\n",
      "Payment_type_Cheque             0\n",
      "Payment_type_Credit card        0\n",
      "Payment_type_Cross-border       0\n",
      "Payment_type_Debit card         0\n",
      "Length: 83, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/data/raw/SAML-D.csv')\n",
    "\n",
    "# Convert datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce')\n",
    "\n",
    "# Feature Engineering\n",
    "sender_diversity = df.groupby('Sender_account')['Receiver_account'].nunique().rename('Recipient_diversity')\n",
    "receiver_diversity = df.groupby('Receiver_account')['Sender_account'].nunique().rename('Sender_diversity')\n",
    "df = df.merge(sender_diversity, on='Sender_account', how='left')\n",
    "df = df.merge(receiver_diversity, on='Receiver_account', how='left')\n",
    "\n",
    "daily_txn_count = df.groupby(['Sender_account', 'Date']).size().rename('Daily_frequency')\n",
    "df = df.merge(daily_txn_count, on=['Sender_account', 'Date'], how='left')\n",
    "avg_velocity = df.groupby('Sender_account')['Daily_frequency'].mean().rename('Avg_velocity')\n",
    "df = df.merge(avg_velocity, on='Sender_account', how='left')\n",
    "\n",
    "inflow = df.groupby('Receiver_account')['Amount'].sum().rename('Total_inflow')\n",
    "outflow = df.groupby('Sender_account')['Amount'].sum().rename('Total_outflow')\n",
    "df = df.merge(inflow, left_on='Sender_account', right_index=True, how='left')\n",
    "df = df.merge(outflow, left_on='Receiver_account', right_index=True, how='left', suffixes=('_inflow', '_outflow'))\n",
    "df['Inflow_Outflow_Ratio'] = df['Total_inflow'] / (df['Total_outflow'] + 1e-6)\n",
    "\n",
    "df = df.sort_values(by=['Sender_account', 'Date', 'Time'])\n",
    "df['Txn_sequence'] = df.groupby('Sender_account').cumcount() + 1\n",
    "df['Rolling_avg_amt'] = df.groupby('Sender_account')['Amount'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "df['Hour'] = df['Time'].dt.hour\n",
    "df['Minute'] = df['Time'].dt.minute\n",
    "df['Weekday'] = df['Date'].dt.weekday\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df = df.drop(columns=['Time', 'Laundering_type'])\n",
    "\n",
    "# Downcast\n",
    "df['Sender_account'] = df['Sender_account'].astype('int32')\n",
    "df['Receiver_account'] = df['Receiver_account'].astype('int32')\n",
    "df['Amount'] = df['Amount'].astype('float32')\n",
    "df['Is_laundering'] = df['Is_laundering'].astype('int8')\n",
    "new_features = ['Recipient_diversity', 'Sender_diversity', 'Daily_frequency', \n",
    "                'Avg_velocity', 'Total_inflow', 'Total_outflow', \n",
    "                'Inflow_Outflow_Ratio', 'Txn_sequence', 'Rolling_avg_amt']\n",
    "for col in new_features:\n",
    "    df[col] = df[col].astype('float32')\n",
    "\n",
    "# Encode categoricals\n",
    "categorical_cols = ['Payment_currency', 'Received_currency', 'Sender_bank_location', \n",
    "                   'Receiver_bank_location', 'Payment_type']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_cols = ['Amount', 'Recipient_diversity', 'Sender_diversity', 'Daily_frequency', \n",
    "                  'Avg_velocity', 'Total_inflow', 'Total_outflow', \n",
    "                  'Inflow_Outflow_Ratio', 'Txn_sequence', 'Rolling_avg_amt', \n",
    "                  'Hour', 'Minute', 'Weekday', 'Day', 'Month', 'Is_laundering']\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "joblib.dump(scaler, '/home/students/Documents/AML CFT_dashboard_project/models/scaler.pkl')\n",
    "\n",
    "# Save encoded columns\n",
    "joblib.dump(df.columns.tolist(), '/home/students/Documents/AML CFT_dashboard_project/models/encoded_columns.pkl')\n",
    "\n",
    "# Handle NaNs\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median(numeric_only=True))\n",
    "\n",
    "# Save\n",
    "output_dir = \"/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "df.to_csv(os.path.join(output_dir, \"cleaned_data.csv\"), index=False)\n",
    "print(\"Cleaned data saved.\")\n",
    "\n",
    "# Verify\n",
    "saved_df = pd.read_csv(os.path.join(output_dir, \"cleaned_data.csv\"))\n",
    "print(\"Saved data shape:\", saved_df.shape)\n",
    "print(\"Saved data columns:\", saved_df.columns.tolist())\n",
    "print(\"Missing values:\", saved_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f9cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9000a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a70ad995",
   "metadata": {},
   "source": [
    "Unsupervised Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb9f728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Fitting HDBSCAN on sample size: 10000\n",
      "HDBSCAN Classification Report:\n",
      "Confusion Matrix:\n",
      "[[8762 1226]\n",
      " [   6    6]]\n",
      "Model saved: /home/students/Documents/AML CFT_dashboard_project/models/hdbscan_model_20250627_234910.pkl\n",
      "Predictions saved: /home/students/Documents/AML CFT_dashboard_project/models/hdbscan_predictions_20250627_234910.csv\n",
      "Confusion matrix saved: /home/students/Documents/AML CFT_dashboard_project/models/hdbscan_confusion_matrix_20250627_234910.csv\n",
      "Classification report saved: /home/students/Documents/AML CFT_dashboard_project/models/hdbscan_report_20250627_234910.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93      9988\n",
      "           1       0.00      0.50      0.01        12\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.50      0.69      0.47     10000\n",
      "weighted avg       1.00      0.88      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # HDBSCAN\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "log = logging.info\n",
    "\n",
    "# Load cleaned data\n",
    "data_path = '/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/cleaned_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Sample data\n",
    "sample_size = min(10000, len(df))\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Features and target\n",
    "y_true = df_sample['Is_laundering'].values\n",
    "X = df_sample.drop(columns=['Is_laundering', 'Date'])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit HDBSCAN\n",
    "log(f\" Fitting HDBSCAN on sample size: {sample_size}\")\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=100, prediction_data=True)\n",
    "cluster_labels = clusterer.fit_predict(X_scaled)\n",
    "anomaly_labels = (cluster_labels == -1).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "report = classification_report(y_true, anomaly_labels, zero_division=0, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_true, anomaly_labels)\n",
    "\n",
    "log(\"HDBSCAN Classification Report:\")\n",
    "print(classification_report(y_true, anomaly_labels, zero_division=0))\n",
    "log(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Timestamped file names\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_path = '/home/students/Documents/AML CFT_dashboard_project/models'\n",
    "model_path = os.path.join(base_path, f'hdbscan_model_{timestamp}.pkl')\n",
    "pred_path = os.path.join(base_path, f'hdbscan_predictions_{timestamp}.csv')\n",
    "conf_path = os.path.join(base_path, f'hdbscan_confusion_matrix_{timestamp}.csv')\n",
    "json_path = os.path.join(base_path, f'hdbscan_report_{timestamp}.json')\n",
    "\n",
    "# Save model\n",
    "joblib.dump(clusterer, model_path)\n",
    "log(f\"Model saved: {model_path}\")\n",
    "\n",
    "# Save predictions\n",
    "pred_df = pd.DataFrame({\n",
    "    'True_Label': y_true,\n",
    "    'HDBSCAN_Prediction': anomaly_labels,\n",
    "    'Outlier_Score': clusterer.outlier_scores_\n",
    "})\n",
    "pred_df.to_csv(pred_path, index=False)\n",
    "log(f\"Predictions saved: {pred_path}\")\n",
    "\n",
    "# Save confusion matrix\n",
    "pd.DataFrame(conf_matrix, index=['Actual 0', 'Actual 1'], columns=['Pred 0', 'Pred 1']).to_csv(conf_path)\n",
    "log(f\"Confusion matrix saved: {conf_path}\")\n",
    "\n",
    "# Save report as JSON (for dashboard or API)\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(report, f, indent=4)\n",
    "log(f\"Classification report saved: {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "778306a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (9504852, 83)\n",
      "Target shape: (9504852,)\n",
      "Best Parameters: {'contamination': 0.01, 'n_estimators': 100}\n",
      "Isolation Forest Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    949502\n",
      "           1       0.01      0.11      0.02       983\n",
      "\n",
      "    accuracy                           0.99    950485\n",
      "   macro avg       0.51      0.55      0.51    950485\n",
      "weighted avg       1.00      0.99      0.99    950485\n",
      "\n",
      "Model saved to: /home/students/Documents/AML CFT_dashboard_project/models/isolation_forest.pkl\n"
     ]
    }
   ],
   "source": [
    "## isolation forest \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/cleaned_data.csv')\n",
    "\n",
    "# Select features (include Is_laundering, exclude Date)\n",
    "features = [col for col in df.columns if col not in ['Date']]\n",
    "X = df[features]\n",
    "y = df['Is_laundering']\n",
    "\n",
    "# Load preprocessing artifacts\n",
    "encoded_columns = joblib.load('/home/students/Documents/AML CFT_dashboard_project/models/encoded_columns.pkl')\n",
    "\n",
    "# Align features with saved columns\n",
    "X = X.reindex(columns=encoded_columns, fill_value=0)\n",
    "\n",
    "print('Data shape:', X.shape)\n",
    "print('Target shape:', y.shape)\n",
    "\n",
    "# Sample 10% of data\n",
    "X_sample = X.sample(frac=0.1, random_state=42)\n",
    "y_sample = y[X_sample.index]\n",
    "\n",
    "# Clear memory\n",
    "del df, X, y\n",
    "\n",
    "# Initialize Isolation Forest\n",
    "iso_forest = IsolationForest(random_state=42)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'contamination': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Fine-tune model\n",
    "best_score = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = IsolationForest(**params, random_state=42)\n",
    "    model.fit(X_sample)\n",
    "    y_pred = model.predict(X_sample)\n",
    "    y_pred = np.where(y_pred == -1, 1, 0)  # Convert -1 (anomaly) to 1, 1 (normal) to 0\n",
    "    report = classification_report(y_sample, y_pred, output_dict=True, zero_division=0)\n",
    "    f1_score = report['1']['f1-score']\n",
    "    if f1_score > best_score:\n",
    "        best_score = f1_score\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "\n",
    "# Evaluate best model\n",
    "y_pred = best_model.predict(X_sample)\n",
    "y_pred = np.where(y_pred == -1, 1, 0)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print('Isolation Forest Results:')\n",
    "print(classification_report(y_sample, y_pred, zero_division=0))\n",
    "\n",
    "# Save model\n",
    "output_dir = '/home/students/Documents/AML CFT_dashboard_project/models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(best_model, os.path.join(output_dir, 'isolation_forest.pkl'))\n",
    "print(f'Model saved to: {os.path.join(output_dir, \"isolation_forest.pkl\")}')\n",
    "\n",
    "# Clear memory\n",
    "del X_sample, y_sample, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f315ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (9504852, 83)\n",
      "Target shape: (9504852,)\n",
      "Best Parameters: {'gamma': 'scale', 'nu': 0.01}\n",
      "One-Class SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     94958\n",
      "           1       0.00      0.02      0.00        91\n",
      "\n",
      "    accuracy                           0.99     95049\n",
      "   macro avg       0.50      0.51      0.50     95049\n",
      "weighted avg       1.00      0.99      0.99     95049\n",
      "\n",
      "Model saved to: /home/students/Documents/AML CFT_dashboard_project/models/one_class_svm.pkl\n"
     ]
    }
   ],
   "source": [
    "## One class SVM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/cleaned_data.csv')\n",
    "\n",
    "# Select features (include Is_laundering, exclude Date)\n",
    "features = [col for col in df.columns if col not in ['Date']]\n",
    "X = df[features]\n",
    "y = df['Is_laundering']\n",
    "\n",
    "# Load preprocessing artifacts\n",
    "encoded_columns = joblib.load('/home/students/Documents/AML CFT_dashboard_project/models/encoded_columns.pkl')\n",
    "\n",
    "# Align features with saved columns\n",
    "X = X.reindex(columns=encoded_columns, fill_value=0)\n",
    "\n",
    "print('Data shape:', X.shape)\n",
    "print('Target shape:', y.shape)\n",
    "\n",
    "# Sample 1% of data\n",
    "X_sample = X.sample(frac=0.01, random_state=42)\n",
    "y_sample = y[X_sample.index]\n",
    "\n",
    "# Clear memory\n",
    "del df, X, y\n",
    "\n",
    "# Initialize One-Class SVM\n",
    "oc_svm = OneClassSVM(kernel='rbf')\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'nu': [0.001, 0.01],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Fine-tune model\n",
    "best_score = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = OneClassSVM(**params)\n",
    "    model.fit(X_sample)\n",
    "    y_pred = model.predict(X_sample)\n",
    "    y_pred = np.where(y_pred == -1, 1, 0)  # Convert -1 (anomaly) to 1, 1 (normal) to 0\n",
    "    report = classification_report(y_sample, y_pred, output_dict=True, zero_division=0)\n",
    "    f1_score = report['1']['f1-score']\n",
    "    if f1_score > best_score:\n",
    "        best_score = f1_score\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "\n",
    "# Evaluate best model\n",
    "y_pred = best_model.predict(X_sample)\n",
    "y_pred = np.where(y_pred == -1, 1, 0)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print('One-Class SVM Results:')\n",
    "print(classification_report(y_sample, y_pred, zero_division=0))\n",
    "\n",
    "# Save model\n",
    "output_dir = '/home/students/Documents/AML CFT_dashboard_project/models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(best_model, os.path.join(output_dir, 'one_class_svm.pkl'))\n",
    "print(f'Model saved to: {os.path.join(output_dir, \"one_class_svm.pkl\")}')\n",
    "\n",
    "# Clear memory\n",
    "del X_sample, y_sample, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a94743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (9504852, 83)\n",
      "Target shape: (9504852,)\n",
      "Best Parameters: {'n_neighbors': 20, 'contamination': 0.01}\n",
      "Local Outlier Factor Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     94958\n",
      "           1       0.00      0.00      0.00        91\n",
      "\n",
      "    accuracy                           0.99     95049\n",
      "   macro avg       0.50      0.50      0.50     95049\n",
      "weighted avg       1.00      0.99      0.99     95049\n",
      "\n",
      "Model saved to: /home/students/Documents/AML CFT_dashboard_project/models/local_outlier_factor.pkl\n",
      "Feature names saved to: /home/students/Documents/AML CFT_dashboard_project/models/lof_feature_names.pkl\n"
     ]
    }
   ],
   "source": [
    "# Local Outlier Factor (LOF)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/cleaned_data.csv')\n",
    "\n",
    "# Select features (exclude Date)\n",
    "features = [col for col in df.columns if col not in ['Date']]\n",
    "X = df[features]\n",
    "y = df['Is_laundering']\n",
    "\n",
    "# Load preprocessing artifacts\n",
    "encoded_columns = joblib.load('/home/students/Documents/AML CFT_dashboard_project/models/encoded_columns.pkl')\n",
    "\n",
    "# Align features with saved columns\n",
    "X = X.reindex(columns=encoded_columns, fill_value=0)\n",
    "\n",
    "print('Data shape:', X.shape)\n",
    "print('Target shape:', y.shape)\n",
    "\n",
    "# Sample 1% of data due to LOF's computational intensity\n",
    "X_sample = X.sample(frac=0.01, random_state=42)\n",
    "y_sample = y[X_sample.index]\n",
    "\n",
    "# Convert to NumPy array to avoid feature name warning\n",
    "X_sample_np = X_sample.to_numpy()\n",
    "feature_names = X_sample.columns  # Store for reference\n",
    "\n",
    "# Clear memory\n",
    "del df, X, y\n",
    "\n",
    "# Initialize LOF with default parameters\n",
    "lof = LocalOutlierFactor(novelty=True)  # novelty=True for dashboard predictions\n",
    "default_params = {'n_neighbors': 20, 'contamination': 0.01}\n",
    "best_model = LocalOutlierFactor(n_neighbors=20, contamination=0.01, novelty=True)\n",
    "best_model.fit(X_sample_np)  # Default model to avoid NoneType\n",
    "best_score = 0\n",
    "best_params = default_params\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [20, 50],\n",
    "    'contamination': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Fine-tune model\n",
    "for params in ParameterGrid(param_grid):\n",
    "    try:\n",
    "        model = LocalOutlierFactor(n_neighbors=params['n_neighbors'], contamination=params['contamination'], novelty=True)\n",
    "        model.fit(X_sample_np)\n",
    "        y_pred = model.predict(X_sample_np)\n",
    "        y_pred = np.where(y_pred == -1, 1, 0)  # Convert -1 (outlier) to 1, 1 (inlier) to 0\n",
    "        report = classification_report(y_sample, y_pred, output_dict=True, zero_division=0)\n",
    "        f1_score = report['1']['f1-score']\n",
    "        if f1_score > best_score:\n",
    "            best_score = f1_score\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "    except Exception as e:\n",
    "        print(f\"Error with parameters {params}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Evaluate best model\n",
    "y_pred = best_model.predict(X_sample_np)\n",
    "y_pred = np.where(y_pred == -1, 1, 0)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print('Local Outlier Factor Results:')\n",
    "print(classification_report(y_sample, y_pred, zero_division=0))\n",
    "\n",
    "# Save model and feature names\n",
    "output_dir = '/home/students/Documents/AML CFT_dashboard_project/models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(best_model, os.path.join(output_dir, 'local_outlier_factor.pkl'))\n",
    "joblib.dump(feature_names, os.path.join(output_dir, 'lof_feature_names.pkl'))\n",
    "print(f'Model saved to: {os.path.join(output_dir, \"local_outlier_factor.pkl\")}')\n",
    "print(f'Feature names saved to: {os.path.join(output_dir, \"lof_feature_names.pkl\")}')\n",
    "\n",
    "# Clear memory\n",
    "del X_sample, X_sample_np, y_sample, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "685a27b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (9504852, 83)\n",
      "Target shape: (9504852,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/students/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/students/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/students/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'contamination': 0.01, 'n_clusters': 10}\n",
      "Best Threshold: 920900863.5641096\n",
      "K-Means Clustering Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    949502\n",
      "           1       0.00      0.01      0.00       983\n",
      "\n",
      "    accuracy                           0.99    950485\n",
      "   macro avg       0.50      0.50      0.50    950485\n",
      "weighted avg       1.00      0.99      0.99    950485\n",
      "\n",
      "Model saved to: /home/students/Documents/AML CFT_dashboard_project/models/kmeans_clustering.pkl\n",
      "Threshold saved to: /home/students/Documents/AML CFT_dashboard_project/models/kmeans_threshold.pkl\n"
     ]
    }
   ],
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/cleaned_data.csv')\n",
    "\n",
    "# Select features (include Is_laundering, exclude Date)\n",
    "features = [col for col in df.columns if col not in ['Date']]\n",
    "X = df[features]\n",
    "y = df['Is_laundering']\n",
    "\n",
    "# Load preprocessing artifacts\n",
    "encoded_columns = joblib.load('/home/students/Documents/AML CFT_dashboard_project/models/encoded_columns.pkl')\n",
    "\n",
    "# Align features with saved columns\n",
    "X = X.reindex(columns=encoded_columns, fill_value=0)\n",
    "\n",
    "print('Data shape:', X.shape)\n",
    "print('Target shape:', y.shape)\n",
    "\n",
    "# Sample 10% of data\n",
    "X_sample = X.sample(frac=0.1, random_state=42)\n",
    "y_sample = y[X_sample.index]\n",
    "\n",
    "# Clear memory\n",
    "del df, X, y\n",
    "\n",
    "# Initialize K-Means\n",
    "kmeans = KMeans(random_state=42)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_clusters': [5, 10],\n",
    "    'contamination': [0.001, 0.01]  # Threshold for flagging outliers\n",
    "}\n",
    "\n",
    "# Fine-tune model\n",
    "best_score = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "best_threshold = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = KMeans(n_clusters=params['n_clusters'], random_state=42)\n",
    "    model.fit(X_sample)\n",
    "    # Compute distances to nearest centroid\n",
    "    distances = np.min(model.transform(X_sample), axis=1)\n",
    "    # Determine threshold for outliers based on contamination\n",
    "    threshold = np.percentile(distances, 100 * (1 - params['contamination']))\n",
    "    y_pred = np.where(distances > threshold, 1, 0)  # Far from centroid = 1 (anomaly)\n",
    "    report = classification_report(y_sample, y_pred, output_dict=True, zero_division=0)\n",
    "    f1_score = report['1']['f1-score']\n",
    "    if f1_score > best_score:\n",
    "        best_score = f1_score\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Evaluate best model\n",
    "distances = np.min(best_model.transform(X_sample), axis=1)\n",
    "y_pred = np.where(distances > best_threshold, 1, 0)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print('K-Means Clustering Results:')\n",
    "print(classification_report(y_sample, y_pred, zero_division=0))\n",
    "\n",
    "# Save model and threshold\n",
    "output_dir = '/home/students/Documents/AML CFT_dashboard_project/models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(best_model, os.path.join(output_dir, 'kmeans_clustering.pkl'))\n",
    "joblib.dump(best_threshold, os.path.join(output_dir, 'kmeans_threshold.pkl'))\n",
    "print(f'Model saved to: {os.path.join(output_dir, \"kmeans_clustering.pkl\")}')\n",
    "print(f'Threshold saved to: {os.path.join(output_dir, \"kmeans_threshold.pkl\")}')\n",
    "\n",
    "# Clear memory\n",
    "del X_sample, y_sample, best_model, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d240abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93e5eaac",
   "metadata": {},
   "source": [
    "Supervised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbbe6c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (9504852, 81)\n",
      "Target shape: (9504852,)\n",
      "Train shape: (1329302, 20)\n",
      "Test shape: (569702, 20)\n",
      "Balanced class counts: Is_laundering\n",
      "0    949502\n",
      "1    949502\n",
      "Name: count, dtype: int64\n",
      "Top features: ['Total_outflow', 'Sender_diversity', 'Recipient_diversity', 'Inflow_Outflow_Ratio', 'Daily_frequency', 'Total_inflow', 'Avg_velocity', 'Payment_type_Cross-border', 'Txn_sequence', 'Payment_type_Cash Deposit', 'Payment_type_Cash Withdrawal', 'Weekday', 'Rolling_avg_amt', 'Amount', 'Day', 'Month', 'Received_currency_Euro', 'Receiver_bank_location_UK', 'Sender_account', 'Received_currency_Moroccan dirham']\n"
     ]
    }
   ],
   "source": [
    "# supervised data preparation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/cleaned_data.csv')\n",
    "\n",
    "# Select features (exclude Date, Is_laundering)\n",
    "features = [col for col in df.columns if col not in ['Date', 'Is_laundering']]\n",
    "X = df[features]\n",
    "y = df['Is_laundering']\n",
    "\n",
    "print('Data shape:', X.shape)\n",
    "print('Target shape:', y.shape)\n",
    "\n",
    "# Sample 10% of data\n",
    "X_sample = X.sample(frac=0.1, random_state=42)\n",
    "y_sample = y[X_sample.index]\n",
    "\n",
    "# Apply SMOTE for class balance\n",
    "smote = SMOTE(sampling_strategy=1.0, random_state=42)  # 1:1 balance\n",
    "X_resampled, y_resampled = smote.fit_resample(X_sample, y_sample)\n",
    "\n",
    "# Feature selection (train temporary Random Forest)\n",
    "rf_temp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_temp.fit(X_resampled, y_resampled)\n",
    "importances = pd.Series(rf_temp.feature_importances_, index=X_resampled.columns)\n",
    "top_features = importances.nlargest(20).index\n",
    "X_resampled = X_resampled[top_features]\n",
    "X_sample = X_sample[top_features]  # For consistency in later models\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# Save train/test data\n",
    "output_dir = '/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "pd.DataFrame(X_train).to_csv(os.path.join(output_dir, 'X_train.csv'), index=False)\n",
    "pd.DataFrame(X_test).to_csv(os.path.join(output_dir, 'X_test.csv'), index=False)\n",
    "pd.Series(y_train).to_csv(os.path.join(output_dir, 'y_train.csv'), index=False)\n",
    "pd.Series(y_test).to_csv(os.path.join(output_dir, 'y_test.csv'), index=False)\n",
    "\n",
    "# Save top features for dashboard consistency\n",
    "joblib.dump(top_features.tolist(), os.path.join(output_dir, 'top_features.pkl'))\n",
    "\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape:', X_test.shape)\n",
    "print('Balanced class counts:', pd.Series(y_resampled).value_counts())\n",
    "print('Top features:', top_features.tolist())\n",
    "\n",
    "# Clear memory\n",
    "del df, X, y, X_sample, y_sample, X_resampled, y_resampled, X_train, X_test, y_train, y_test, rf_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ed2136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1329302, 20)\n",
      "Test shape: (569702, 20)\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=13.0min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=13.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=13.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=13.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 6.0min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 6.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 6.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 5.6min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=12.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=12.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 5.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 5.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=18.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=18.2min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=17.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=19.2min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=18.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=19.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=18.2min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=18.1min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=13.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=13.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=18.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=13.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 5.8min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 5.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 5.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=12.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=18.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=19.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=18.9min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=11.4min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=11.3min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=16.3min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=16.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=14.8min\n",
      "Best Parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 20}\n",
      "Random Forest Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    284851\n",
      "           1       1.00      1.00      1.00    284851\n",
      "\n",
      "    accuracy                           1.00    569702\n",
      "   macro avg       1.00      1.00      1.00    569702\n",
      "weighted avg       1.00      1.00      1.00    569702\n",
      "\n",
      "Model saved to: /home/students/Documents/AML CFT_dashboard_project/models/random_forest.pkl\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load train/test data\n",
    "X_train = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/y_train.csv')\n",
    "y_test = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/y_test.csv')\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "# Load top features\n",
    "top_features = joblib.load('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/top_features.pkl')\n",
    "X_train = X_train[top_features]\n",
    "X_test = X_test[top_features]\n",
    "\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape:', X_test.shape)\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Fine-tune with RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=12, scoring='f1', cv=3, n_jobs=-1, random_state=42, verbose=2)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate best model\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "print(f'Best Parameters: {random_search.best_params_}')\n",
    "print('Random Forest Results:')\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Create plots directory\n",
    "plots_dir = '/home/students/Documents/AML CFT_dashboard_project/plots'\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Plot feature importance\n",
    "importances = pd.Series(best_rf.feature_importances_, index=X_train.columns)\n",
    "importances.sort_values(ascending=False).plot(kind='bar', title='Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, 'rf_feature_importance.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save model\n",
    "output_dir = '/home/students/Documents/AML CFT_dashboard_project/models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(best_rf, os.path.join(output_dir, 'random_forest.pkl'))\n",
    "print(f'Model saved to: {os.path.join(output_dir, \"random_forest.pkl\")}')\n",
    "\n",
    "# Clear memory\n",
    "del X_train, X_test, y_train, y_test, best_rf, random_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f367f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1329302, 20)\n",
      "Test shape: (569702, 20)\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] END ..................C=0.001, penalty=l2, solver=lbfgs; total time=   2.1s\n",
      "[CV] END ..................C=0.001, penalty=l2, solver=lbfgs; total time=   2.5s\n",
      "[CV] END ..................C=0.001, penalty=l2, solver=lbfgs; total time=   2.6s\n",
      "[CV] END ..............C=0.001, penalty=l2, solver=liblinear; total time=   2.4s\n",
      "[CV] END ..............C=0.001, penalty=l2, solver=liblinear; total time=   3.3s\n",
      "[CV] END ..............C=0.001, penalty=l2, solver=liblinear; total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=28.4min\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=28.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=28.5min\n",
      "[CV] END ...................C=0.01, penalty=l2, solver=lbfgs; total time=   1.2s\n",
      "[CV] END ...................C=0.01, penalty=l2, solver=lbfgs; total time=   1.2s\n",
      "[CV] END ...................C=0.01, penalty=l2, solver=lbfgs; total time=   1.2s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   1.7s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   2.0s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   1.7s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   2.0s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=28.7min\n",
      "[CV] END ....................C=100, penalty=l2, solver=lbfgs; total time=   1.3s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   1.8s\n",
      "[CV] END ....................C=100, penalty=l2, solver=lbfgs; total time=   1.3s\n",
      "[CV] END ....................C=100, penalty=l2, solver=lbfgs; total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=27.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=28.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time=27.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time=27.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time=37.7min\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   1.5s\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   1.5s\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................C=0.001, penalty=l2, solver=saga; total time=37.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................C=0.001, penalty=l2, solver=saga; total time=37.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................C=0.001, penalty=l2, solver=saga; total time=38.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time=24.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time=24.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time=25.5min\n",
      "Best Parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.001}\n",
      "Logistic Regression Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.57      0.54    284851\n",
      "           1       0.51      0.45      0.48    284851\n",
      "\n",
      "    accuracy                           0.51    569702\n",
      "   macro avg       0.51      0.51      0.51    569702\n",
      "weighted avg       0.51      0.51      0.51    569702\n",
      "\n",
      "Model saved to: /home/students/Documents/AML CFT_dashboard_project/models/logistic_regression.pkl\n"
     ]
    }
   ],
   "source": [
    "# Rogistic regression \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load train/test data\n",
    "X_train = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/y_train.csv')\n",
    "y_test = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/y_test.csv')\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "# Load top features\n",
    "top_features = joblib.load('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/top_features.pkl')\n",
    "X_train = X_train[top_features]\n",
    "X_test = X_test[top_features]\n",
    "\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape:', X_test.shape)\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "lr = LogisticRegression(random_state=42, max_iter=2000)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga'],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "\n",
    "# Fine-tune with RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(lr, param_distributions=param_grid, n_iter=12, scoring='f1', cv=3, n_jobs=-1, random_state=42, verbose=2)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate best model\n",
    "best_lr = random_search.best_estimator_\n",
    "y_pred = best_lr.predict(X_test)\n",
    "print(f'Best Parameters: {random_search.best_params_}')\n",
    "print('Logistic Regression Results:')\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Create plots directory\n",
    "plots_dir = '/home/students/Documents/AML CFT_dashboard_project/plots'\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Plot coefficients\n",
    "coefficients = pd.Series(best_lr.coef_[0], index=X_train.columns)\n",
    "coefficients.sort_values(ascending=False).plot(kind='bar', title='Logistic Regression Coefficients')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, 'lr_coefficients.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save model\n",
    "output_dir = '/home/students/Documents/AML CFT_dashboard_project/models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(best_lr, os.path.join(output_dir, 'logistic_regression.pkl'))\n",
    "print(f'Model saved to: {os.path.join(output_dir, \"logistic_regression.pkl\")}')\n",
    "\n",
    "# Clear memory\n",
    "del X_train, X_test, y_train, y_test, best_lr, random_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e16050f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "[CV] END .............................C=0.001, kernel=linear; total time= 3.9min\n",
      "[CV] END .............................C=0.001, kernel=linear; total time= 4.0min\n",
      "[CV] END .............................C=0.001, kernel=linear; total time= 4.1min\n",
      "[CV] END ................................C=0.001, kernel=rbf; total time=10.7min\n",
      "[CV] END ................................C=0.001, kernel=rbf; total time= 9.9min\n",
      "[CV] END ................................C=0.001, kernel=rbf; total time=10.0min\n",
      "[CV] END ...............................C=0.001, kernel=poly; total time=10.1min\n",
      "[CV] END ..............................C=0.01, kernel=linear; total time= 2.8min\n",
      "[CV] END ..............................C=0.01, kernel=linear; total time= 2.7min\n",
      "[CV] END ..............................C=0.01, kernel=linear; total time= 2.9min\n",
      "[CV] END ...............................C=0.001, kernel=poly; total time= 9.7min\n",
      "[CV] END .................................C=0.01, kernel=rbf; total time= 4.6min\n",
      "[CV] END ...............................C=0.001, kernel=poly; total time= 9.7min\n",
      "[CV] END .................................C=0.01, kernel=rbf; total time= 4.7min\n",
      "[CV] END .................................C=0.01, kernel=rbf; total time= 4.6min\n",
      "[CV] END ...............................C=0.1, kernel=linear; total time= 2.5min\n",
      "[CV] END ................................C=0.01, kernel=poly; total time= 7.4min\n",
      "[CV] END ................................C=0.01, kernel=poly; total time= 5.2min\n",
      "[CV] END ...............................C=0.1, kernel=linear; total time= 2.6min\n",
      "[CV] END ................................C=0.01, kernel=poly; total time= 7.7min\n",
      "[CV] END ...............................C=0.1, kernel=linear; total time= 2.5min\n",
      "[CV] END ..................................C=0.1, kernel=rbf; total time= 2.4min\n",
      "[CV] END ..................................C=0.1, kernel=rbf; total time= 2.5min\n",
      "[CV] END ..................................C=0.1, kernel=rbf; total time= 2.4min\n",
      "[CV] END .................................C=0.1, kernel=poly; total time= 2.9min\n",
      "[CV] END .................................C=0.1, kernel=poly; total time= 2.9min\n",
      "[CV] END .................................C=0.1, kernel=poly; total time= 2.8min\n",
      "[CV] END ....................................C=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .................................C=1, kernel=linear; total time= 3.6min\n",
      "[CV] END ....................................C=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .................................C=1, kernel=linear; total time= 3.8min\n",
      "[CV] END ....................................C=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .................................C=1, kernel=linear; total time= 3.7min\n",
      "[CV] END ...................................C=1, kernel=poly; total time= 1.6min\n",
      "[CV] END ...................................C=1, kernel=poly; total time= 1.7min\n",
      "[CV] END ...................................C=1, kernel=poly; total time= 1.7min\n",
      "[CV] END ...................................C=10, kernel=rbf; total time=  41.8s\n",
      "[CV] END ...................................C=10, kernel=rbf; total time=  41.1s\n",
      "[CV] END ...................................C=10, kernel=rbf; total time=  47.7s\n",
      "[CV] END ..................................C=10, kernel=poly; total time= 1.1min\n",
      "[CV] END ..................................C=10, kernel=poly; total time= 1.1min\n",
      "[CV] END ..................................C=10, kernel=poly; total time= 1.1min\n",
      "[CV] END ................................C=10, kernel=linear; total time=12.4min\n",
      "[CV] END ................................C=10, kernel=linear; total time=12.7min\n",
      "[CV] END ................................C=10, kernel=linear; total time=12.1min\n",
      "Best Parameters: {'C': 10, 'kernel': 'rbf'}\n",
      "SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     24849\n",
      "           1       1.00      1.00      1.00     25151\n",
      "\n",
      "    accuracy                           1.00     50000\n",
      "   macro avg       1.00      1.00      1.00     50000\n",
      "weighted avg       1.00      1.00      1.00     50000\n",
      "\n",
      "Model saved to: /home/students/Documents/AML CFT_dashboard_project/models/svm_classifier.pkl\n"
     ]
    }
   ],
   "source": [
    "# SVM with Kernel Variants\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "X_train = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/y_train.csv')\n",
    "y_test = pd.read_csv('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/y_test.csv')\n",
    "\n",
    "# Flatten target arrays\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "# Load top features\n",
    "top_features = joblib.load('/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/top_features.pkl')\n",
    "X_train = X_train[top_features]\n",
    "X_test = X_test[top_features]\n",
    "\n",
    "# Downsample\n",
    "X_train = X_train.sample(n=100000, random_state=42)\n",
    "y_train = y_train[X_train.index]\n",
    "X_test = X_test.sample(n=50000, random_state=42)\n",
    "y_test = y_test[X_test.index]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize base SVM\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "# Expanded hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to explore all combinations\n",
    "grid_search = GridSearchCV(\n",
    "    svm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred = best_svm.predict(X_test)\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print('SVM Results:')\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Save plot of coefficients (only works if linear)\n",
    "if best_svm.kernel == 'linear':\n",
    "    coefficients = pd.Series(best_svm.coef_[0], index=top_features)\n",
    "    coefficients.sort_values(ascending=False).plot(kind='bar', title='SVM Linear Kernel Coefficients')\n",
    "    plt.tight_layout()\n",
    "    plots_dir = '/home/students/Documents/AML CFT_dashboard_project/plots'\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(plots_dir, 'svm_coefficients.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Save best model\n",
    "output_dir = '/home/students/Documents/AML CFT_dashboard_project/models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(best_svm, os.path.join(output_dir, 'svm_classifier.pkl'))\n",
    "print(f'Model saved to: {os.path.join(output_dir, \"svm_classifier.pkl\")}')\n",
    "\n",
    "# Clean up\n",
    "del X_train, X_test, y_train, y_test, best_svm, grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8767e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define Paths\n",
    "data_path = \"/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/cleaned_data.csv\"\n",
    "top_features_path = \"/home/students/Documents/AML CFT_dashboard_project/notebooks/data/processed/top_features.pkl\"\n",
    "output_path = \"/home/students/Documents/AML CFT_dashboard_project/app/sample_for_dashboard.csv\"\n",
    "\n",
    "# Load Cleaned Data\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"✅ Loaded cleaned_data.csv:\", df.shape)\n",
    "\n",
    "# Load Top Features\n",
    "top_features = joblib.load(top_features_path)\n",
    "print(\"✅ Loaded top_features.pkl with\", len(top_features), \"features:\", top_features)\n",
    "\n",
    "# Select Columns (Date, top features, Is_laundering)\n",
    "columns_to_keep = ['Date'] + top_features + ['Is_laundering'] if 'Is_laundering' in df.columns else ['Date'] + top_features\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Verify Features\n",
    "missing_cols = set(top_features) - set(df.columns)\n",
    "if missing_cols:\n",
    "    print(f\"⚠️ Missing features in data: {missing_cols}\")\n",
    "    for col in missing_cols:\n",
    "        df[col] = 0\n",
    "\n",
    "# Ensure correct order\n",
    "df = df[['Date'] + top_features + (['Is_laundering'] if 'Is_laundering' in df.columns else [])]\n",
    "\n",
    "# Numerical columns for scaling (only those in top_features)\n",
    "numerical_cols = [col for col in top_features if col in [\n",
    "    'Amount', 'Recipient_diversity', 'Sender_diversity', 'Daily_frequency', \n",
    "    'Avg_velocity', 'Total_inflow', 'Total_outflow', 'Inflow_Outflow_Ratio', \n",
    "    'Txn_sequence', 'Rolling_avg_amt', 'Weekday', 'Day', 'Month'\n",
    "]]\n",
    "print(\"✅ Numerical columns for scaling:\", numerical_cols)\n",
    "\n",
    "# Fit a new scaler on the numerical columns in top_features\n",
    "if numerical_cols:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[numerical_cols])\n",
    "    df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
    "    # Save the new scaler for consistency\n",
    "    joblib.dump(scaler, \"/home/students/Documents/AML CFT_dashboard_project/models/scaler_sample.pkl\")\n",
    "    print(\"✅ Saved new scaler_sample.pkl\")\n",
    "\n",
    "# Handle any NaNs (should be none, but ensure robustness)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Sample 10,000 rows\n",
    "sample_df = df.sample(n=10000, random_state=42)\n",
    "print(\"✅ Sample shape:\", sample_df.shape)\n",
    "\n",
    "# Save to CSV (No Index)\n",
    "sample_df.to_csv(output_path, index=False)\n",
    "print(f\"✅ Sample saved for dashboard upload: {output_path}\")\n",
    "\n",
    "# Verify\n",
    "saved_df = pd.read_csv(output_path)\n",
    "print(\"✅ Saved data columns:\", saved_df.columns.tolist())\n",
    "print(\"✅ Missing values:\", saved_df.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims_cv_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
